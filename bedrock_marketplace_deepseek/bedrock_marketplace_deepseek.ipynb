{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bedrock Marketplace DeepSeek\n",
    "\n",
    "In this notebook, we'll use Bedrock Marketplace API to deploy a `DeepSeek-R1-Distill-Llama-8B` model to a Bedrock Marketplace Deployment endpoint.\n",
    "\n",
    "**Pre-requisites:**\n",
    "- Make sure the model is available in the region which you are deploying the model. We'll use `us-east-2` in this notebook.\n",
    "- Make sure you have enough account level quota for `ml.g6.2xlarge for endpoint usage`. If not, you can request a quota increase from the AWS console here [SageMake Service Quotas](https://us-east-2.console.aws.amazon.com/servicequotas/home/services/sagemaker/quotas?region=us-east-2)\n",
    "- Upgrade your `boto3` library to the latest version. You can do this by running `!pip install boto3 --upgrade` in a code cell. This notebook is tested with `boto3==1.36.14`.\n",
    "- An IAM role which gives access to model artifacts in AWS ECR registeries and S3, and deploy the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an IAM role for Deployment\n",
    "You can reuse this role for multiple deployments.\n",
    "\n",
    "**IMPORTANT**: The IAM policy used here is for demonstration purposes only. In production environments, always follow the principle of least privilege by granting only the minimum necessary permissions required for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "role_name = \"AmazonSageMakerExecutionRoleForBedrockMarketplace\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define iam and trust policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": \"sagemaker.amazonaws.com\"},\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Read inline policy from file\n",
    "with open(\"iam_policy.json\", \"r\") as f:\n",
    "    inline_policy = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the role if it doesn't exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role exists, using it...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    role = iam.get_role(RoleName=role_name)\n",
    "    print(\"Role exists, using it...\")\n",
    "except iam.exceptions.NoSuchEntityException:\n",
    "    print(\"Role does not exist, creating it...\")\n",
    "    role = iam.create_role(\n",
    "        RoleName=role_name, AssumeRolePolicyDocument=json.dumps(trust_policy)\n",
    "    )\n",
    "    iam.put_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyName=\"inline-policy\",\n",
    "        PolicyDocument=json.dumps(inline_policy),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-east-2\"\n",
    "model_arn = \"arn:aws:sagemaker:us-east-2:aws:hub-content/SageMakerPublicHub/Model/deepseek-llm-r1-distill-llama-8b/1.0.0\"\n",
    "endpoint_name = \"deepseek-r1-llama-8b-ep\"\n",
    "instance_type = \"ml.g6.2xlarge\"\n",
    "\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "bedrock = boto3.client(\"bedrock\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_endpoint(\n",
    "    model_arn, endpoint_name, instance_type, role_arn, timeout\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a Bedrock Marketplace Endpoint for a model. After creating\n",
    "    a new endpoint, this function will poll the endpoint until it is in service\n",
    "    or until the timeout is reached.\n",
    "    \"\"\"\n",
    "    endpoint = bedrock.create_marketplace_model_endpoint(\n",
    "        modelSourceIdentifier=model_arn,\n",
    "        endpointConfig={\n",
    "            \"sageMaker\": {\n",
    "                \"initialInstanceCount\": 1,\n",
    "                \"instanceType\": instance_type,\n",
    "                \"executionRole\": role_arn,\n",
    "            }\n",
    "        },\n",
    "        endpointName=endpoint_name,\n",
    "    )\n",
    "    endpoint_arn = endpoint[\"marketplaceModelEndpoint\"][\"endpointArn\"]\n",
    "\n",
    "    # Poll the endpoint until it is in service\n",
    "    print(\"Endpoint is created, waiting for it to be in service...\")\n",
    "    for s in range(timeout // 5):\n",
    "        endpoint = bedrock.get_marketplace_model_endpoint(\n",
    "            endpointArn=endpoint_arn\n",
    "        )\n",
    "        status = endpoint[\"marketplaceModelEndpoint\"][\"endpointStatus\"]\n",
    "        if status == \"InService\":\n",
    "            print(\"Endpoint is in service.\")\n",
    "            break\n",
    "        time.sleep(5)\n",
    "    else:\n",
    "        print(\"Timeout: Endpoint is not in service yet.\")\n",
    "\n",
    "    return endpoint_arn\n",
    "\n",
    "\n",
    "def get_or_create_endpoint(\n",
    "    model_arn, endpoint_name, instance_type, role_arn, timeout=360\n",
    "):\n",
    "    \"\"\"\n",
    "    Get or create a Bedrock Marketplace Endpoint for a model and return the\n",
    "    endpoint ARN.\n",
    "    \"\"\"\n",
    "    endpoints = bedrock.list_marketplace_model_endpoints(\n",
    "        modelSourceEquals=model_arn\n",
    "    )[\"marketplaceModelEndpoints\"]\n",
    "\n",
    "    if endpoints:\n",
    "        print(\"Endpoint exists, using it...\")\n",
    "        return endpoints[0][\"endpointArn\"]\n",
    "\n",
    "    print(\"Endpoint does not exist, creating it...\")\n",
    "    endpoint_arn = create_endpoint(\n",
    "        model_arn, endpoint_name, instance_type, role_arn, timeout\n",
    "    )\n",
    "\n",
    "    return endpoint_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the endpoint ARN. If the endpoint exists, we'll reuse it otherwise we'll create a new endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint does not exist, creating it...\n",
      "Endpoint is created, waiting for it to be in service...\n",
      "Endpoint is in service.\n"
     ]
    }
   ],
   "source": [
    "endpoint_arn = get_or_create_endpoint(\n",
    "    model_arn=model_arn,\n",
    "    endpoint_name=endpoint_name,\n",
    "    instance_type=instance_type,\n",
    "    role_arn=role[\"Role\"][\"Arn\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepseek_r1_chat(\n",
    "    bedrock_runtime,\n",
    "    endpoint_arn,\n",
    "    prompt,\n",
    "    temperature=0.6,\n",
    "    max_tokens=1000,\n",
    "    top_p=0.9,\n",
    "):\n",
    "    \"\"\"Constructs the payload for the deepseek model, sends it to the endpoint,\n",
    "    and returns the response.\n",
    "\n",
    "    Returns:\n",
    "      The response is string made of two parts. Model's chain of thoughts\n",
    "      which is contained within the <think></think> tags, and the final\n",
    "      answer.\n",
    "    \"\"\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": f\"\"\"You are an AI assistant. Do as the user asks.\n",
    "    ### Instruction: {prompt}\n",
    "    ### Response: <think>\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": max_tokens,\n",
    "            \"top_p\": top_p,\n",
    "            \"temperature\": temperature,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Invoke model\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=endpoint_arn, body=json.dumps(payload)\n",
    "    )\n",
    "\n",
    "    output = None\n",
    "    if response:\n",
    "        body = response.get(\"body\")\n",
    "        if body:\n",
    "            body = json.loads(body.read().decode(\"utf-8\"))\n",
    "            output = body.get(\"generated_text\")\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepseek_r1_parse_output(response):\n",
    "    \"\"\"Parses the response from the deepseek model and returns the output.\n",
    "\n",
    "    Returns:\n",
    "      Dict[str, str]: The parsed output with the keys \"cot\" and \"answer\".\n",
    "    \"\"\"\n",
    "    output = {\n",
    "        \"cot\": \"\",\n",
    "        \"answer\": \"\",\n",
    "    }\n",
    "\n",
    "    if not response:\n",
    "        return output\n",
    "\n",
    "    # Extract content after \"### Response:\"\n",
    "    content = response.split(\"### Response:\")[1].strip()\n",
    "\n",
    "    # Extract content between <think> and </think>\n",
    "    cot_match = re.search(r\"<think>(.*?)</think>\", content, re.DOTALL)\n",
    "    if cot_match:\n",
    "        output[\"cot\"] = cot_match.group(1).strip()\n",
    "\n",
    "    # Extract final answer which comes after </think>\n",
    "    if \"</think>\" in content:\n",
    "        output[\"answer\"] = content.split(\"</think>\", 1)[1].strip()\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"If Alice has 6 apples and gives half to Bob, who then eats 2 apples and shares the rest equally with his sister, how many apples does Bob's sister receive?\"\n",
    "\n",
    "response = deepseek_r1_chat(bedrock_runtime, endpoint_arn, prompt)\n",
    "output = deepseek_r1_parse_output(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Response:\n",
      "You are an AI assistant. Do as the user asks.\n",
      "    ### Instruction: If Alice has 6 apples and gives half to Bob, who then eats 2 apples and shares the rest equally with his sister, how many apples does Bob's sister receive?\n",
      "    ### Response: <think>\n",
      "Okay, so I need to figure out how many apples Bob's sister receives after all these transactions. Let me break it down step by step.\n",
      "\n",
      "First, Alice starts with 6 apples. She gives half to Bob. Hmm, half of 6 is 3, right? So Bob gets 3 apples. That leaves Alice with 3 apples as well because she gave away half.\n",
      "\n",
      "Now, Bob has 3 apples. He eats 2 of them. So, if he eats 2, how many does he have left? Let me subtract 2 from 3. That gives me 1 apple remaining. So Bob now has 1 apple left.\n",
      "\n",
      "Next, Bob shares the rest equally with his sister. The \"rest\" here refers to the apples he has after eating, which is 1 apple. He wants to share this equally, so he'll split it into two parts. If he shares 1 apple equally between himself and his sister, each would get half of that apple. But wait, you can't really split an apple into halves physically, but in terms of sharing, it's like each gets 0.5 apples. However, the question is asking how many apples his sister receives, so I think we're supposed to consider whole apples here.\n",
      "\n",
      "Wait, maybe I made a mistake. Let me go back. Alice gives half to Bob, so she gives 3 apples, keeping 3. Bob gets 3, eats 2, so he has 1 left. Then he shares that 1 apple with his sister. If he shares equally, each gets 0.5 apples. But since we can't have half apples in this context, perhaps the question expects us to round down or consider it as a fraction.\n",
      "\n",
      "Alternatively, maybe I should think differently. After Bob eats 2, he has 1 apple. He shares the rest equally with his sister, so he divides that 1 apple between two people. Each would get 0.5, but since apples are whole, perhaps the sister gets 0 apples? That doesn't make sense. Or maybe the question allows for fractional apples in the answer.\n",
      "\n",
      "Wait, maybe I should present the answer as half an apple. So the sister gets 0.5 apples. But the question might expect a whole number. Let me check again.\n",
      "\n",
      "Alice gives Bob 3, Bob eats 2, leaving him with 1. He shares that 1 equally, so each gets 0.5. So the sister gets 0.5 apples. Alternatively, if the sharing is done before eating, but that doesn't make sense because Bob eats after receiving.\n",
      "\n",
      "I think the correct answer is that the sister gets half an apple, so 0.5 apples. But sometimes in these problems, they might expect you to consider that you can't split apples, so maybe the sister gets 0. But I think the more accurate answer is 0.5.\n",
      "</think>\n",
      "\n",
      "Bob's sister receives 0.5 apples.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Chain of Thought:\n",
      "Okay, so I need to figure out how many apples Bob's sister receives after all these transactions. Let me break it down step by step.\n",
      "\n",
      "First, Alice starts with 6 apples. She gives half to Bob. Hmm, half of 6 is 3, right? So Bob gets 3 apples. That leaves Alice with 3 apples as well because she gave away half.\n",
      "\n",
      "Now, Bob has 3 apples. He eats 2 of them. So, if he eats 2, how many does he have left? Let me subtract 2 from 3. That gives me 1 apple remaining. So Bob now has 1 apple left.\n",
      "\n",
      "Next, Bob shares the rest equally with his sister. The \"rest\" here refers to the apples he has after eating, which is 1 apple. He wants to share this equally, so he'll split it into two parts. If he shares 1 apple equally between himself and his sister, each would get half of that apple. But wait, you can't really split an apple into halves physically, but in terms of sharing, it's like each gets 0.5 apples. However, the question is asking how many apples his sister receives, so I think we're supposed to consider whole apples here.\n",
      "\n",
      "Wait, maybe I made a mistake. Let me go back. Alice gives half to Bob, so she gives 3 apples, keeping 3. Bob gets 3, eats 2, so he has 1 left. Then he shares that 1 apple with his sister. If he shares equally, each gets 0.5 apples. But since we can't have half apples in this context, perhaps the question expects us to round down or consider it as a fraction.\n",
      "\n",
      "Alternatively, maybe I should think differently. After Bob eats 2, he has 1 apple. He shares the rest equally with his sister, so he divides that 1 apple between two people. Each would get 0.5, but since apples are whole, perhaps the sister gets 0 apples? That doesn't make sense. Or maybe the question allows for fractional apples in the answer.\n",
      "\n",
      "Wait, maybe I should present the answer as half an apple. So the sister gets 0.5 apples. But the question might expect a whole number. Let me check again.\n",
      "\n",
      "Alice gives Bob 3, Bob eats 2, leaving him with 1. He shares that 1 equally, so each gets 0.5. So the sister gets 0.5 apples. Alternatively, if the sharing is done before eating, but that doesn't make sense because Bob eats after receiving.\n",
      "\n",
      "I think the correct answer is that the sister gets half an apple, so 0.5 apples. But sometimes in these problems, they might expect you to consider that you can't split apples, so maybe the sister gets 0. But I think the more accurate answer is 0.5.\n",
      "\n",
      "Final Answer:\n",
      "Bob's sister receives 0.5 apples.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Raw Response:\\n{response}\\n\")\n",
    "print(\"-\" * 80)\n",
    "print(\n",
    "    f\"Chain of Thought:\\n{output[\"cot\"]}\\n\\n\"\n",
    "    f\"Final Answer:\\n{output[\"answer\"]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting endpoint...\n"
     ]
    }
   ],
   "source": [
    "response = bedrock.delete_marketplace_model_endpoint(endpointArn=endpoint_arn)\n",
    "if response[\"ResponseMetadata\"][\"HTTPStatusCode\"] == 200:\n",
    "    print(\"Deleting endpoint...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
